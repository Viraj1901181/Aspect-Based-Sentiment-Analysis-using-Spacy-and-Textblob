{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate sentiment for specific topics or attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GradientBoostingClassifier',\n",
       " 'GradientBoostingRegressor',\n",
       " 'LazyImport',\n",
       " 'OneHotEncoder',\n",
       " 'Path',\n",
       " 'RandomForestClassifier',\n",
       " 'RandomForestRegressor',\n",
       " 'SparkContext',\n",
       " 'TSNE',\n",
       " 'TfidfVectorizer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_importable',\n",
       " '_imports',\n",
       " '_jupyter_labextension_paths',\n",
       " '_jupyter_nbextension_paths',\n",
       " 'active_imports',\n",
       " 'alt',\n",
       " 'bokeh',\n",
       " 'dash',\n",
       " 'dd',\n",
       " 'dt',\n",
       " 'gensim',\n",
       " 'get_user_symbols',\n",
       " 'glob',\n",
       " 'go',\n",
       " 'import_symbol',\n",
       " 'install_extensions',\n",
       " 'install_labextension',\n",
       " 'install_nbextension',\n",
       " 'keras',\n",
       " 'lazy_imports',\n",
       " 'lgb',\n",
       " 'load_workbook',\n",
       " 'mpl',\n",
       " 'nltk',\n",
       " 'np',\n",
       " 'os',\n",
       " 'pd',\n",
       " 'pickle',\n",
       " 'plt',\n",
       " 'px',\n",
       " 'py',\n",
       " 'pydot',\n",
       " 'pyforest_imports',\n",
       " 're',\n",
       " 'sklearn',\n",
       " 'sns',\n",
       " 'spacy',\n",
       " 'statistics',\n",
       " 'svm',\n",
       " 'sys',\n",
       " 'tf',\n",
       " 'tqdm',\n",
       " 'train_test_split',\n",
       " 'user_specific_imports',\n",
       " 'user_symbols',\n",
       " 'utils',\n",
       " 'wr',\n",
       " 'xgb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One of the most common goals with NLP is to analyze text and extract insights. You can find countless tutorials on how to perform sentiment analysis, but the typical way that’s used is not always enough.\n",
    "\n",
    "### When you pass a sentence like this.\n",
    "\n",
    "We had some amazing food yesterday. But the next day was very boring.\n",
    "\n",
    "#### A sentence-by-sentence sentiment analysis algorithm would produce something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We had some amazing food yesterday. POSITIVE\n",
    "\n",
    "## But the next day was very boring. NEGATIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But there are times when you want your sentiment analysis to be aspect-based, or otherwise called topic-based.\n",
    "\n",
    "In this article, we will build a very simplistic aspect-based sentiment analysis that’s able to pick up generic concepts and understand the sentiments around them. In our previous example, that would mean something like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazing food POSITIVE\n",
    "\n",
    "### Very boring day NEGATIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aspects, or topics, in this case, are food and day. By performing sentiment analysis based on aspects we analyze large pieces of text and extract insights.\n",
    "\n",
    "For example, if you monitor customer reviews, or call transcripts, you can look for aspects that have some sentiment attached and extract insights on how to improve.\n",
    "\n",
    "For this article, we will be using spacy, a natural language processing library in Python along with Textblob which offers simple tools for sentiment analysis and text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get started by importing spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also define a few simple test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'The food we had yesterday was delicious',\n",
    "  'My time in Italy was very enjoyable',\n",
    "  'I found the meal to be tasty',\n",
    "  'The internet was slow.',\n",
    "  'Our experience was suboptimal'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first goal is to split our sentences in a way so that we have the target aspects (e.g. food) and their sentiment descriptions (e.g. delicious)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det food NOUN DET []\n",
      "food nsubj was AUX NOUN [The, had]\n",
      "we nsubj had AUX PRON []\n",
      "had relcl food NOUN AUX [we, yesterday]\n",
      "yesterday npadvmod had AUX NOUN []\n",
      "was ROOT was AUX AUX [food, delicious]\n",
      "delicious acomp was AUX ADJ []\n",
      "My poss time NOUN DET []\n",
      "time nsubj was AUX NOUN [My, in]\n",
      "in prep time NOUN ADP [Italy]\n",
      "Italy pobj in ADP PROPN []\n",
      "was ROOT was AUX AUX [time, enjoyable]\n",
      "very advmod enjoyable ADJ ADV []\n",
      "enjoyable acomp was AUX ADJ [very]\n",
      "I nsubj found VERB PRON []\n",
      "found ROOT found VERB VERB [I, be]\n",
      "the det meal NOUN DET []\n",
      "meal nsubj be AUX NOUN [the]\n",
      "to aux be AUX PART []\n",
      "be ccomp found VERB AUX [meal, to, tasty]\n",
      "tasty acomp be AUX ADJ []\n",
      "The det internet NOUN DET []\n",
      "internet nsubj was AUX NOUN [The]\n",
      "was ROOT was AUX AUX [internet, slow, .]\n",
      "slow acomp was AUX ADJ []\n",
      ". punct was AUX PUNCT []\n",
      "Our poss experience NOUN DET []\n",
      "experience nsubj was AUX NOUN [Our]\n",
      "was ROOT was AUX AUX [experience, suboptimal]\n",
      "suboptimal acomp was AUX ADJ []\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "      token.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token inside our sentences, we can see the dependency thanks to spacy’s dependency parsing and the POS (Part-Of-Speech) tags. We’re also paying attention to the child tokens, so that we’re able to pick up intensifiers such as “very”, “quite”, and more.\n",
    "\n",
    "Disclaimer: Our current simplistic algorithm may not be able to pick up semantically important information such as the “not” in “not great” at the moment. That would be crucial to account for in a real-life application.\n",
    "\n",
    "Let’s see how to pick up the sentiment descriptions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "My time in Italy was very enjoyable\n",
      "enjoyable\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "The internet was slow.\n",
      "slow\n",
      "Our experience was suboptimal\n",
      "suboptimal\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      descriptive_term = token\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our simplistic algorithm picks up all the descriptive adjectives such as delicious, enjoyable, and tasty. But what’s currently missing are intensifiers, like “very” for example.\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "My time in Italy was very enjoyable\n",
      "very enjoyable\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "The internet was slow.\n",
      "slow\n",
      "Our experience was suboptimal\n",
      "suboptimal\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this time around we picked up very enjoyable as well. Our simplistic algorithm is able to pick up adverbs. It checks for child tokens for each adjective and picks up the adverbs such as “very”, “quite”, etc.\n",
    "\n",
    "In a regular scenario, we would need to catch negations such as “not” as well, but this is outside the scope of this article. But you are encouraged to practice and make it more advanced afterward if you’d like.\n",
    "\n",
    "We’re now ready to identify the targets that are being described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': 'delicious'}, {'aspect': 'time', 'description': 'very enjoyable'}, {'aspect': 'meal', 'description': 'tasty'}, {'aspect': 'internet', 'description': 'slow'}, {'aspect': 'experience', 'description': 'suboptimal'}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  target = ''\n",
    "  for token in doc:\n",
    "    if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "      target = token.text\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  aspects.append({'aspect': target,\n",
    "    'description': descriptive_term})\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our solution is starting to look more complete. We’re able to pick up aspects, even though our application doesn’t “know” anything beforehand. We haven’t hardcoded the aspects such as “food”, “time”, or “meal”. And we also haven’t hardcoded the adjectives such as “tasty”, “slow”, or “enjoyable”.\n",
    "\n",
    "Our application picks them up based on the simple rules that we set.\n",
    "\n",
    "There are times when you may want to find the topics first and then identify them in your text while ignoring topics or aspects that are not that common.\n",
    "\n",
    "To do that, you would need to work on Topic Modeling before moving on to the sentiment analysis part of the solution. There’s a great guide on Towards Data Science that explains the Latent Dirichlet Allocation which you can use for Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we successfully extracted the aspects and descriptions, it’s time to classify them as positive or negative. \n",
    "\n",
    "The goal here is to help the computer understand that tasty food is positive, while slow internet is negative. Computers don’t understand English, so we will need to try a few things before we have a working solution.\n",
    "\n",
    "We will start off by using the default TextBlob sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': 'delicious', 'sentiment': Sentiment(polarity=1.0, subjectivity=1.0)}, {'aspect': 'time', 'description': 'very enjoyable', 'sentiment': Sentiment(polarity=0.65, subjectivity=0.78)}, {'aspect': 'meal', 'description': 'tasty', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'internet', 'description': 'slow', 'sentiment': Sentiment(polarity=-0.30000000000000004, subjectivity=0.39999999999999997)}, {'aspect': 'experience', 'description': 'suboptimal', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "  aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is a library that offers sentiment analysis out of the box. It has a bag-of-words approach, meaning that it has a list of words such as “good”, “bad”, and “great” that have a sentiment score attached to them. It is also able to pick up modifiers (such as “not”) and intensifiers (such as “very”) that affect the sentiment score.\n",
    "\n",
    "If we look at our results, I can say it’s definitely not looking bad! I would agree with all of them, but the only problem is that tasty and suboptimal are considered neutral. It seems that they’re not part of TextBlob’s dictionary and as such, they are not picked up.\n",
    "\n",
    "Another potential issue is that some descriptive terms or adjectives can be positive in some cases and negative in others, depending on the word they’re describing. The default algorithm used by TextBlob is not able to know that cold weather can be neutral, cold food can be negative while a cold drink can be positive.\n",
    "\n",
    "The good thing is that TextBlob allows you to train a NaiveBayesClassifier using a very simple syntax that’s easy for anyone to understand, which we will use to improve our sentiment analysis.\n",
    "\n",
    "To be able to use it though, you will need to execute the following to download the required corpora: python -m textblob.download_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "# We train the NaivesBayesClassifier\n",
    "train = [\n",
    "  ('Slow internet.', 'negative'),\n",
    "  ('Delicious food', 'positive'),\n",
    "  ('Suboptimal experience', 'negative'),\n",
    "  ('Very enjoyable time', 'positive'),\n",
    "  ('delicious food.', 'neg')\n",
    "]\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then we try to classify some sample sentences.\n",
    "blob = TextBlob(\"Delicious food. Very Slow internet. Suboptimal experience. Enjoyable food.\", classifier=cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious food.\n",
      "positive\n",
      "Very Slow internet.\n",
      "negative\n",
      "Suboptimal experience.\n",
      "negative\n",
      "Enjoyable food.\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "for s in blob.sentences:\n",
    "  print(s)\n",
    "  print(s.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the sentences we passed are not exactly what we used in the training examples, but it’s still able to correctly predict the sentiments of all the phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion, Scope and Further Work Possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this solution as a starting point for a more complex aspect-based sentiment analysis solution. To do that you would need to improve the dependency parsing process to extract more accurate information, but also more types of data. A great way to do that is using spacy’s DependencyMatcher which allows you to match patterns using custom rules.\n",
    "\n",
    "As for the sentiment analysis part, ideally, you want to label a lot of data so that you can create more advanced classifiers with a higher amount of accuracy. There are tons of binary (or categorical when you include neutral in the mix) classifications that you can perform using keras, TensorFlow, or other machine learning libraries and tools.\n",
    "\n",
    "If you have pre-labeled data that’s very helpful. If you don’t, you can create an initial analysis using a simple tool like TextBlob, and then instead of deciding on the sentiment for each phrase, you can choose if you agree with TextBlob or not which is a lot quicker than deciding the sentiment from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
